{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams['figure.figsize'] = [14, 8]\n\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nimport gc\nimport os\nimport pyarrow as pa\nimport re\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\nfrom tqdm.auto import trange\nfrom typing import Iterator, List\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, Sampler\nfrom transformers import AutoTokenizer, AutoModel\n\nfrom custom_classes import ContrastSampler, DataManager, Trainer, TrainerA, WeightedCosineSimilarityLoss, ContrastLoss2\nfrom custom_classes import INT2LABEL as categories","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-02-22T12:07:42.088922Z","iopub.execute_input":"2023-02-22T12:07:42.089575Z","iopub.status.idle":"2023-02-22T12:07:43.9213Z","shell.execute_reply.started":"2023-02-22T12:07:42.0895Z","shell.execute_reply":"2023-02-22T12:07:43.920142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\ndevice","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-02-22T12:07:43.938575Z","iopub.execute_input":"2023-02-22T12:07:43.941023Z","iopub.status.idle":"2023-02-22T12:07:43.991Z","shell.execute_reply.started":"2023-02-22T12:07:43.940985Z","shell.execute_reply":"2023-02-22T12:07:43.989892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data_path = 'data'\ndata_path = '../input/semeval/data'\nDEV = True\n#model_name = 'sentence-transformers/all-mpnet-base-v2'\n#model_name = 'sentence-transformers/all-MiniLM-L6-v2'\nmodel_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n#model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\nmodel_sampler = 'contrast'\nN_EPOCHS = 1000\nN_FINETUNE_EPOCHS = 50\nN_EPOCHS_BEFORE_FINETUNE = 50\nN_POST_FINETUNE_EPOCHS = 50\nMODEL_BATCH_SIZE = 26\nHEAD_BATCH_SIZE = 200\nMIN_SAMPLES_FROM_CLASS = 1\nHEAD_LR = 1e-3\nHEAD_GAMMA = .99\nMODEL_LR = 2e-5\nBETA = 0.01\nMODEL_GAMMA = .98\nVALIDATE_EVERY = 1\nCHECKPOINT_EVERY = 10\nEARLIEST_CHECKPOINT = 1000","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-02-22T12:07:44.868278Z","iopub.execute_input":"2023-02-22T12:07:44.868564Z","iopub.status.idle":"2023-02-22T12:07:44.875296Z","shell.execute_reply.started":"2023-02-22T12:07:44.868534Z","shell.execute_reply":"2023-02-22T12:07:44.874279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModel.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:07:44.876776Z","iopub.execute_input":"2023-02-22T12:07:44.877833Z","iopub.status.idle":"2023-02-22T12:07:47.214509Z","shell.execute_reply.started":"2023-02-22T12:07:44.877798Z","shell.execute_reply":"2023-02-22T12:07:47.213506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\nall_langs = ['en', 'ge', 'fr', 'it', 'ru', 'po']\ndatamanager = DataManager(\n    tokenizer=tokenizer,\n    data_dir=data_path,\n    use_dev=DEV,\n    languages_for_head_eval=all_langs,\n    languages_for_head_train=all_langs,\n    languages_for_contrastive=all_langs,\n)\nN_CLASSES = datamanager.num_classes\nmetrics = list()\nreference_list = list()","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:07:47.216147Z","iopub.execute_input":"2023-02-22T12:07:47.216878Z","iopub.status.idle":"2023-02-22T12:07:52.000007Z","shell.execute_reply.started":"2023-02-22T12:07:47.216819Z","shell.execute_reply":"2023-02-22T12:07:51.998996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_DIM = model.embeddings.word_embeddings.embedding_dim\nhead = nn.Sequential(\n    nn.Linear(EMBEDDING_DIM, 256),\n    nn.Dropout(),\n    nn.ReLU(),\n    nn.Linear(256, 256),\n    nn.Dropout(),\n    nn.ReLU(),\n    nn.Linear(256, N_CLASSES),\n    nn.Dropout(),\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:07:52.001711Z","iopub.execute_input":"2023-02-22T12:07:52.003216Z","iopub.status.idle":"2023-02-22T12:07:52.011813Z","shell.execute_reply.started":"2023-02-22T12:07:52.003176Z","shell.execute_reply":"2023-02-22T12:07:52.010883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_and_write_embeddings(trainer, dataset, filename):\n    embeddings = trainer.compute_embeddings(dataset)\n    with open(filename, 'wb') as f:\n        pickle.dump(embeddings, f)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:07:52.013097Z","iopub.execute_input":"2023-02-22T12:07:52.013561Z","iopub.status.idle":"2023-02-22T12:07:52.022193Z","shell.execute_reply.started":"2023-02-22T12:07:52.013521Z","shell.execute_reply":"2023-02-22T12:07:52.021079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_contrastive = datamanager.get_contrastive_dataset()\ndataset_head_train = datamanager.get_head_train_dataset()\ndataset_head_eval = datamanager.get_head_eval_dataset()\ntrainer = TrainerA(\n    model=model,\n    head=head,\n    device=device,\n    head_loss=nn.BCEWithLogitsLoss(),\n    model_loss=WeightedCosineSimilarityLoss(N_CLASSES),\n    model_dataset=dataset_contrastive,\n    head_dataset=dataset_head_train,\n    eval_dataset=dataset_head_eval,        \n    n_classes=N_CLASSES,\n    model_loader_type=model_sampler,\n    train_head_batch_size=HEAD_BATCH_SIZE,\n    train_model_batch_size=MODEL_BATCH_SIZE,\n    head_lr=HEAD_LR,\n    model_lr=MODEL_LR,\n    head_gamma=HEAD_GAMMA,\n    model_gamma=MODEL_GAMMA,\n    beta=BETA,\n    min_samples_from_class=MIN_SAMPLES_FROM_CLASS,\n    validate_every_n_epochs=VALIDATE_EVERY,\n    checkpoint_every_n_epochs=CHECKPOINT_EVERY,\n    earliest_checkpoint=EARLIEST_CHECKPOINT,\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:07:52.023379Z","iopub.execute_input":"2023-02-22T12:07:52.024132Z","iopub.status.idle":"2023-02-22T12:08:12.525279Z","shell.execute_reply.started":"2023-02-22T12:07:52.024029Z","shell.execute_reply":"2023-02-22T12:08:12.524229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Full","metadata":{}},{"cell_type":"code","source":"reference_list.append(dataset_head_eval)\ntrainer.train_head(N_EPOCHS_BEFORE_FINETUNE)\ntrainer.train_joint(N_FINETUNE_EPOCHS)\n#compute_and_write_embeddings(trainer, dataset_head_eval, 'dev_embeddings/embeddings_00.pickle')\n#for epoch in range(N_FINETUNE_EPOCHS):\n#    trainer.train_joint(1)\n#    compute_and_write_embeddings(trainer, dataset_head_eval, f'dev_embeddings/embeddings_{str(epoch+1).zfill(2)}.pickle')\ntrainer.train_head(N_POST_FINETUNE_EPOCHS)\ntrainer.save_hparams('minilm-dev-analysis-hparams')\ntrainer.save_log_dict('minilm-dev-analysis-logdict')\ntrainer.save_checkpoint('joint-contrast-50', 49)\nmetrics.append(trainer.log_dict)\ntrainer.plot_metrics(trainer.log_dict, 1)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:08:23.053158Z","iopub.execute_input":"2023-02-22T12:08:23.05354Z","iopub.status.idle":"2023-02-22T12:47:32.160341Z","shell.execute_reply.started":"2023-02-22T12:08:23.053507Z","shell.execute_reply":"2023-02-22T12:47:32.155882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sanity Check\nlangs = all_langs\nfor lang in langs:\n    dataset_sanity = datamanager._get_single_named_dataset(lang, dev=True)\n    dataset_sanity = datamanager._preprocess_head_dataset(dataset_sanity)\n    embeddings = trainer.compute_embeddings(dataset_sanity)\n    predictions = trainer.predict(embeddings.tensors[0], 'cpu')\n    f1 = f1_score(dataset_sanity['labels'], predictions, average='micro')\n    print(lang, ': ', f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Specific Language","metadata":{}},{"cell_type":"code","source":"for lang in all_langs:\n    print(f'Training {lang}')\n    datamanager = DataManager(\n        tokenizer=tokenizer,\n        data_dir=data_path,\n        use_dev=DEV,\n        languages_for_head_eval=[lang],\n        languages_for_head_train=[lang],\n        languages_for_contrastive=[lang],\n    )\n    dataset_contrastive = datamanager.get_contrastive_dataset()\n    dataset_head_train = datamanager.get_head_train_dataset()\n    dataset_head_eval = datamanager.get_head_eval_dataset()\n    trainer = TrainerA(\n        model=model,\n        head=head,\n        device=device,\n        head_loss=nn.BCEWithLogitsLoss(),\n        model_loss=WeightedCosineSimilarityLoss(N_CLASSES),\n        model_dataset=dataset_contrastive,\n        head_dataset=dataset_head_train,\n        eval_dataset=dataset_head_eval,        \n        n_classes=N_CLASSES,\n        model_loader_type=model_sampler,\n        train_head_batch_size=HEAD_BATCH_SIZE,\n        train_model_batch_size=MODEL_BATCH_SIZE,\n        head_lr=HEAD_LR,\n        model_lr=MODEL_LR,\n        head_gamma=HEAD_GAMMA,\n        model_gamma=MODEL_GAMMA,\n        beta=BETA,\n        min_samples_from_class=MIN_SAMPLES_FROM_CLASS,\n        validate_every_n_epochs=VALIDATE_EVERY,\n        checkpoint_every_n_epochs=CHECKPOINT_EVERY,\n        earliest_checkpoint=10000,\n    )\n    \n    \n    # Loading checkpoint\n    trainer.load_from_checkpoint('joint-contrast-50')\n    #trainer.load_from_checkpoint('dev_embeddings/joint-50')\n    head = nn.Sequential(\n        nn.Linear(EMBEDDING_DIM, 256),\n        nn.Dropout(),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.Dropout(),\n        nn.ReLU(),\n        nn.Linear(256, N_CLASSES),\n        nn.Dropout(),\n    )\n    head_optimizer = AdamW(head.parameters(), lr=HEAD_LR)\n    trainer.set_head(head, head_optimizer)\n\n\n    # Training\n    trainer.train_head(N_EPOCHS_BEFORE_FINETUNE)\n    trainer.train_joint(N_FINETUNE_EPOCHS)\n    trainer.train_head(N_POST_FINETUNE_EPOCHS)\n    trainer.plot_metrics(trainer.log_dict, 1)\n    print('MicroF1@100:  ', trainer.log_dict['microf1'][99])\n    print('MicroF1@last: ', trainer.log_dict['microf1'][-1])\n    #trainer.save_checkpoint(f'{lang}_final_model')\n    \n    \n    # Sanity check\n    #dataset_sanity = datamanager._get_single_named_dataset(lang, dev=True)\n    #dataset_sanity = datamanager._preprocess_head_dataset(dataset_sanity)\n    #embeddings = trainer.compute_embeddings(dataset_sanity)\n    #predictions = trainer.predict(embeddings.tensors[0], 'cpu')\n    #f1 = f1_score(dataset_sanity['labels'], predictions, average='micro')\n    #print('  ', lang, ': ', f1)\n\n    \n    # Write Predictions\n    #print('  Writing prediction file.')\n    #datamanager.predict_and_write(\n    #    trainer,\n    #    articles_dir=f'{data_path}/{lang}/test-articles-subtask-2',\n    #    output_file=f'predictions_{lang}.csv'\n    #)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T12:49:20.312671Z","iopub.execute_input":"2023-02-22T12:49:20.313133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Average Max MicroF1: {np.mean(np.array([np.max(m[\"microf1\"]) for m in metrics]))}')\nprint(f'Average Max MacroF1: {np.mean(np.array([np.max(m[\"macrof1\"]) for m in metrics]))}')\nprint(f'Average Max Train MicroF1: {np.mean(np.array([np.max(m[\"train_microf1\"]) for m in metrics]))}')\nprint(f'Average Max Train MacroF1: {np.mean(np.array([np.max(m[\"train_macrof1\"]) for m in metrics]))}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for log_dict in metrics:\n    TrainerA.plot_metrics(log_dict, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN_scores = []\nground = reference_list[1]['labels'].numpy()\nfor KNN_preds in metrics[1]['KNNlogits']:\n    KNN_preds = torch.round(KNN_preds).numpy()\n    KNN_scores.append(f1_score(ground, KNN_preds, average='micro'))\nplt.plot(KNN_scores)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainer.log_dict['WCSL'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def per_label_f1(predictions, references):\n    f1 = f1_score(references, predictions, average=None)\n    print(\"f1:\", f1)\n    micro_f1 = f1_score(references, predictions, average=\"micro\")\n    print(\"micro-f1:\", micro_f1)\n    macro_f1 = f1_score(references, predictions, average=\"macro\")\n    print(\"macro-f1:\", macro_f1)\n\n    correct = []\n    label_names = []\n    for c in range(len(categories)):\n        correct.append(f1[c])\n        label_names.append(categories[c])\n    correct = np.array(correct)\n    label_names = np.array(label_names)\n    df_correct_pred = pd.DataFrame({\"f1_score\": correct, \"label_name\": label_names})\n\n    order = sorted(range(len(categories)), key=lambda i: f1[i])\n    return order, sns.barplot(x=\"f1_score\", y=\"label_name\", data=df_correct_pred, order=np.array(categories)[order])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_all(predictions, references):\n    order, _ = per_label_f1(predictions, references)\n    plt.show()\n    \n    pred_heatmap = pd.DataFrame(predictions, columns=categories)\n    correct_predictions = references == predictions\n    false_predictions   = references != predictions\n    pred_heatmap[(correct_predictions & (predictions == 1))] = 3 # correct and one\n    pred_heatmap[(correct_predictions & (predictions == 0))] = 2 # correct and zero\n    pred_heatmap[(false_predictions & (predictions == 1))] = 1   # false and actually zero\n    pred_heatmap[(false_predictions & (predictions == 0))] = 0   # false and actually one\n    \n    pred_heatmap = pred_heatmap.iloc[:,order[::-1]]\n    pred_heatmap['false_predictions'] = false_predictions.sum(axis=1)\n    #pred_heatmap = pred_heatmap.sort_values(by='false_predictions', ascending=False)\n    pred_heatmap = pred_heatmap.sort_values(by=list(pred_heatmap.columns), ascending=False)\n    \n    fig, ax = plt.subplots(figsize=[14, 20])\n    cmap = sns.color_palette(\"coolwarm_r\", 4)\n    sns.heatmap(pred_heatmap.iloc[:,:-1], cmap=cmap)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for r, m in zip(reference_list, metrics):\n    best_epoch = np.argmax(m['microf1'])\n    predictions = np.array([p.cpu().tolist() for p in m['predictions']][best_epoch])\n    references = r['labels'].numpy()\n    plot_all(predictions, references)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stuff","metadata":{}},{"cell_type":"raw","source":"access_token = \"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\"\nmodel.push_to_hub(repo_path_or_name=\"mpnet-multilang-body-wcsl-contrastsampling\",\n                  repo_url=\"https://huggingface.co/Kwin-exe/mpnet-multilang-body-wcsl-contrastsampling\",\n                  use_auth_token=access_token)","metadata":{}},{"cell_type":"raw","source":"from IPython.display import FileLink\nFileLink('embeddings.zip')","metadata":{"execution":{"iopub.status.busy":"2023-02-16T15:32:46.475191Z","iopub.execute_input":"2023-02-16T15:32:46.475838Z","iopub.status.idle":"2023-02-16T15:32:46.481153Z","shell.execute_reply.started":"2023-02-16T15:32:46.4758Z","shell.execute_reply":"2023-02-16T15:32:46.479906Z"}}}]}